{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a UNet Model for Solar Radio Burst Segmentation using PyTorch\n",
    "\n",
    "In this notebook, we demonstrate how to train a UNet model for segmenting solar radio bursts using transfer learning. \n",
    "The training is split into two phases:\n",
    "\n",
    "1. **Phase 1:** Freeze the encoder (pre-trained on ImageNet) and train only the decoder.  \n",
    "2. **Phase 2:** Unfreeze the encoder and fine-tune the entire model using a lower learning rate.\n",
    "\n",
    "We use a combined loss function consisting of binary cross-entropy (BCE) loss and a Jaccard (IOU) loss, and we monitor the IOU and F1 metrics on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Enhanced Loss Function Setup\n",
    "\n",
    "This notebook now supports the new enhanced loss function system with:\n",
    "- **Focal Loss**: For handling class imbalance (radio bursts are rare)\n",
    "- **Boundary Loss**: For improving edge detection accuracy\n",
    "- **Adaptive IoU Loss**: For better overlap quality assessment\n",
    "\n",
    "The enhanced loss functions are automatically optimized for solar radio burst detection!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remiliascarlet/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Enhanced loss functions loaded successfully!\n",
      "üìã Available loss configurations:\n",
      "  ‚Ä¢ balanced: Standard training with roughly balanced positive/negative samples\n",
      "  ‚Ä¢ imbalanced: Training with sparse positive samples (few radio bursts)\n",
      "  ‚Ä¢ noisy: Training data with significant noise and artifacts\n",
      "  ‚Ä¢ boundary_critical: High precision required for burst edge detection\n"
     ]
    }
   ],
   "source": [
    "# Import enhanced loss function utilities (add this to your imports)\n",
    "from train_utils import (\n",
    "    combined_loss, simple_combined_loss, focal_loss, boundary_loss, adaptive_iou_loss\n",
    ")\n",
    "from loss_config_example import get_loss_config_for_scenario\n",
    "from loss_tuner import LossTuner, quick_tune\n",
    "from train_utils import build_deeplabv3\n",
    "\n",
    "print(\"üéØ Enhanced loss functions loaded successfully!\")\n",
    "\n",
    "# Show available loss configurations\n",
    "configs = [\"balanced\", \"imbalanced\", \"noisy\", \"boundary_critical\"]\n",
    "print(\"üìã Available loss configurations:\")\n",
    "for config in configs:\n",
    "    desc = get_loss_config_for_scenario(config)[\"description\"]\n",
    "    print(f\"  ‚Ä¢ {config}: {desc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "üéØ Enhanced loss functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ÂéüÂßãÁöÑimportsÈúÄË¶ÅÊõ¥Êñ∞‰∏∫Ôºö\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# üéØ Import enhanced loss function utilities  \n",
    "from train_utils import (\n",
    "    create_dataset, build_unet, freeze_encoder_weights, unfreeze_encoder_weights,\n",
    "    combined_loss, simple_combined_loss, focal_loss, boundary_loss, adaptive_iou_loss,\n",
    "    compute_metrics, train_one_epoch, validate_one_epoch, adjust_learning_rate, \n",
    "    save_checkpoint, train_model\n",
    ")\n",
    "from loss_config_example import get_loss_config_for_scenario\n",
    "from loss_tuner import LossTuner, quick_tune\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print(\"üéØ Enhanced loss functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Loading and Preprocessing\n",
    "\n",
    "We assume that image and mask CSV files are stored in a single directory.\n",
    "The naming convention is as follows:\n",
    "\n",
    "- For burst slices:\n",
    "       slice_20240608_y155_x270406_SkylineHS.csv\n",
    "       slice_20240608_y155_x270406_SkylineHS_mask.csv\n",
    "\n",
    "- For non-burst slices:\n",
    "       slice_20240420_y0_x3391_PeachMountain_2020_nonburst.csv\n",
    "\n",
    "The `create_dataset` function reads the files, normalizes them to [0, 1], and splits the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (1171, 256, 256, 1)\n",
      "Validation images shape: (293, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/remiliascarlet/Desktop/MDP/transfer_learning/burst_data/csv/saved_slices/25spring'\n",
    "\n",
    "(train_images, train_masks), (val_images, val_masks) = create_dataset(data_dir, img_size=(256, 256), test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create DataLoaders\n",
    "\n",
    "Convert the numpy arrays into PyTorch tensors and create DataLoaders.\n",
    "\n",
    "We also need to permute dimensions from (N, H, W, C) to (N, C, H, W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert numpy arrays to Torch tensors and permute to (N, channels, H, W)\n",
    "train_images_tensor = torch.tensor(train_images).permute(0, 3, 1, 2)\n",
    "train_masks_tensor  = torch.tensor(train_masks).permute(0, 3, 1, 2)\n",
    "\n",
    "val_images_tensor = torch.tensor(val_images).permute(0, 3, 1, 2)\n",
    "val_masks_tensor  = torch.tensor(val_masks).permute(0, 3, 1, 2)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_images_tensor, train_masks_tensor)\n",
    "val_dataset   = TensorDataset(val_images_tensor, val_masks_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Construction\n",
    "\n",
    "Build a UNet model using the `build_unet` function from the segmentation_models library.\n",
    "\n",
    "Here we set `input_shape=(256,256,1)` for grayscale images, `num_classes=1` for binary segmentation,\n",
    "and use pre-trained ImageNet weights with a ResNet34 backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (encoder): ResNetEncoder(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): UnetDecoder(\n",
      "    (center): Identity()\n",
      "    (blocks): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (segmentation_head): SegmentationHead(\n",
      "    (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Activation(\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import segmentation_models_pytorch library if not already installed: pip install segmentation-models-pytorch\n",
    "# model = build_unet(input_shape=(256, 256, 1), num_classes=1, encoder_weights='imagenet')\n",
    "# model.to(device)\n",
    "\n",
    "model = build_deeplabv3(\n",
    "    input_shape=(256, 256, 1), \n",
    "    num_classes=1, \n",
    "    encoder_weights=None,        # üî• Key: No ImageNet pretraining\n",
    "    encoder_name='resnet34'      # Can change to 'resnet18' for faster training\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Enhanced Training with Loss Function Tuning\n",
    "\n",
    "The training functions now automatically use enhanced loss functions optimized for radio burst detection. You can also customize the loss parameters based on your specific needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Loss Function Configuration Options\n",
    "print(\"üéØ Enhanced Loss Function Configuration\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Option 1: Use predefined scenarios (RECOMMENDED)\n",
    "print(\"üìã Option 1: Predefined Scenarios\")\n",
    "scenarios = {\n",
    "    \"imbalanced\": \"Sparse positive samples (recommended for radio bursts)\",  \n",
    "    \"balanced\": \"Roughly balanced positive/negative samples\",\n",
    "    \"noisy\": \"Significant noise and artifacts in data\",\n",
    "    \"boundary_critical\": \"High precision required for edge detection\",\n",
    "    \"original\": \"Use simple BCE+IoU loss (backward compatibility)\"\n",
    "}\n",
    "\n",
    "for scenario, description in scenarios.items():\n",
    "    config = get_loss_config_for_scenario(scenario)\n",
    "    print(f\"  ‚Ä¢ {scenario}: {description}\")\n",
    "    if not config.get('use_simple_loss', False):\n",
    "        print(f\"    Weights: {config['loss_weights']}\")\n",
    "        print(f\"    Focal: {config['focal_params']}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Option 2: Custom parameters\n",
    "print(\"üîß Option 2: Custom Parameters\")\n",
    "print(\"  You can specify custom loss_weights and focal_params in train_model()\")\n",
    "print(\"  Example:\")\n",
    "print(\"    loss_weights = {'focal': 1.5, 'iou': 1.2, 'boundary': 0.3}\")\n",
    "print(\"    focal_params = {'alpha': 0.85, 'gamma': 3.0}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Option 3: Auto-tuning based on validation metrics\n",
    "print(\"üìä Option 3: Auto-tuning (Advanced)\")\n",
    "print(\"  Use LossTuner to get suggestions based on validation metrics\")\n",
    "example_metrics = {'precision': 0.85, 'recall': 0.45, 'f1': 0.59, 'iou': 0.42}\n",
    "print(f\"  Example: {example_metrics}\")\n",
    "\n",
    "# Uncomment to use auto-tuning:\n",
    "# tuner = LossTuner()\n",
    "# suggested = tuner.suggest_parameters(example_metrics)\n",
    "# print(f\"  Suggested: {suggested}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ DeepLabV3+ Training Configuration\n",
    "print(\"‚öôÔ∏è Configuring training parameters for DeepLabV3+...\")\n",
    "\n",
    "# Training parameters optimized for DeepLabV3+\n",
    "training_config_deeplabv3 = {\n",
    "    'initial_lr': 1e-3,       # Standard learning rate\n",
    "    'freeze_epochs': 0,       # No freezing needed (no ImageNet)\n",
    "    'total_epochs': 50,       # Can be increased for better results\n",
    "    'patience': 15,           # More patience due to no pretraining\n",
    "    'loss_config': 'imbalanced'  # Use predefined imbalanced configuration\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration ready:\")\n",
    "print(f\"   Loss weights: {loss_config_deeplabv3}\")\n",
    "print(f\"   Training epochs: {training_config_deeplabv3['total_epochs']}\")\n",
    "print(f\"   No encoder freezing (training from scratch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Enhanced Training with Configurable Loss...\n",
      "============================================================\n",
      "üéØ Using 'imbalanced' loss configuration:\n",
      "   Description: Training with sparse positive samples (few radio bursts)\n",
      "   Loss weights: {'focal': 1.2, 'iou': 1.5, 'boundary': 0.2}\n",
      "   Focal params: {'alpha': 0.8, 'gamma': 2.5}\n",
      "Phase 1: Freezing encoder and training decoder only.\n",
      "Epoch 1/100 - Train Loss: 0.7590 - Val Loss: 0.6816 - IOU: 0.2173 - F1: 0.2679\n",
      "  Loss Components - Focal: 0.0558, IoU: 0.4514, Boundary: 0.0749\n",
      "Checkpoint saved at epoch 1 with best metric 0.6816393701653731 -> ./checkpoints_enhanced/checkpoint_epoch_1_metric_0.6816.pth\n",
      "Epoch 2/100 - Train Loss: 0.6432 - Val Loss: 0.6502 - IOU: 0.2319 - F1: 0.2785\n",
      "  Loss Components - Focal: 0.0702, IoU: 0.3627, Boundary: 0.0747\n",
      "Checkpoint saved at epoch 2 with best metric 0.6502368277625034 -> ./checkpoints_enhanced/checkpoint_epoch_2_metric_0.6502.pth\n",
      "Epoch 3/100 - Train Loss: 0.6297 - Val Loss: 0.6073 - IOU: 0.2331 - F1: 0.2805\n",
      "  Loss Components - Focal: 0.0663, IoU: 0.3567, Boundary: 0.0748\n",
      "Checkpoint saved at epoch 3 with best metric 0.6073463763061323 -> ./checkpoints_enhanced/checkpoint_epoch_3_metric_0.6073.pth\n",
      "Epoch 4/100 - Train Loss: 0.5910 - Val Loss: 0.5811 - IOU: 0.2438 - F1: 0.2899\n",
      "  Loss Components - Focal: 0.0688, IoU: 0.3289, Boundary: 0.0747\n",
      "Checkpoint saved at epoch 4 with best metric 0.5811302128591036 -> ./checkpoints_enhanced/checkpoint_epoch_4_metric_0.5811.pth\n",
      "Epoch 5/100 - Train Loss: 0.5613 - Val Loss: 0.6451 - IOU: 0.2230 - F1: 0.2702\n",
      "  Loss Components - Focal: 0.0709, IoU: 0.3075, Boundary: 0.0747\n",
      "Epoch 6/100 - Train Loss: 0.5831 - Val Loss: 0.5954 - IOU: 0.2297 - F1: 0.2746\n",
      "  Loss Components - Focal: 0.0736, IoU: 0.3199, Boundary: 0.0747\n",
      "Epoch 7/100 - Train Loss: 0.5227 - Val Loss: 0.5879 - IOU: 0.2486 - F1: 0.2942\n",
      "  Loss Components - Focal: 0.0649, IoU: 0.2866, Boundary: 0.0746\n",
      "Epoch 8/100 - Train Loss: 0.4994 - Val Loss: 0.6291 - IOU: 0.2255 - F1: 0.2709\n",
      "  Loss Components - Focal: 0.0651, IoU: 0.2709, Boundary: 0.0746\n",
      "Epoch 9/100 - Train Loss: 0.5164 - Val Loss: 0.5933 - IOU: 0.2387 - F1: 0.2850\n",
      "  Loss Components - Focal: 0.0726, IoU: 0.2763, Boundary: 0.0746\n",
      "Epoch 10/100 - Train Loss: 0.4721 - Val Loss: 0.5676 - IOU: 0.2423 - F1: 0.2880\n",
      "  Loss Components - Focal: 0.0619, IoU: 0.2552, Boundary: 0.0745\n",
      "Checkpoint saved at epoch 10 with best metric 0.5676389273844267 -> ./checkpoints_enhanced/checkpoint_epoch_10_metric_0.5676.pth\n",
      "Epoch 11/100 - Train Loss: 0.4367 - Val Loss: 0.5588 - IOU: 0.2516 - F1: 0.2954\n",
      "  Loss Components - Focal: 0.0570, IoU: 0.2356, Boundary: 0.0745\n",
      "Checkpoint saved at epoch 11 with best metric 0.5588196029788569 -> ./checkpoints_enhanced/checkpoint_epoch_11_metric_0.5588.pth\n",
      "Epoch 12/100 - Train Loss: 0.4339 - Val Loss: 0.5559 - IOU: 0.2544 - F1: 0.2997\n",
      "  Loss Components - Focal: 0.0622, IoU: 0.2295, Boundary: 0.0745\n",
      "Checkpoint saved at epoch 12 with best metric 0.555881181829854 -> ./checkpoints_enhanced/checkpoint_epoch_12_metric_0.5559.pth\n",
      "Epoch 13/100 - Train Loss: 0.4296 - Val Loss: 0.5987 - IOU: 0.2326 - F1: 0.2772\n",
      "  Loss Components - Focal: 0.0603, IoU: 0.2282, Boundary: 0.0745\n",
      "Epoch 14/100 - Train Loss: 0.4695 - Val Loss: 0.5682 - IOU: 0.2480 - F1: 0.2946\n",
      "  Loss Components - Focal: 0.0596, IoU: 0.2554, Boundary: 0.0745\n",
      "Epoch 15/100 - Train Loss: 0.4187 - Val Loss: 0.5677 - IOU: 0.2549 - F1: 0.2996\n",
      "  Loss Components - Focal: 0.0539, IoU: 0.2261, Boundary: 0.0745\n",
      "Epoch 16/100 - Train Loss: 0.3901 - Val Loss: 0.5715 - IOU: 0.2522 - F1: 0.2982\n",
      "  Loss Components - Focal: 0.0554, IoU: 0.2059, Boundary: 0.0744\n",
      "Epoch 17/100 - Train Loss: 0.3720 - Val Loss: 0.5554 - IOU: 0.2483 - F1: 0.2912\n",
      "  Loss Components - Focal: 0.0588, IoU: 0.1910, Boundary: 0.0745\n",
      "Checkpoint saved at epoch 17 with best metric 0.5554379099293759 -> ./checkpoints_enhanced/checkpoint_epoch_17_metric_0.5554.pth\n",
      "Epoch 18/100 - Train Loss: 0.3748 - Val Loss: 0.6514 - IOU: 0.2273 - F1: 0.2715\n",
      "  Loss Components - Focal: 0.0614, IoU: 0.1909, Boundary: 0.0745\n",
      "Epoch 19/100 - Train Loss: 0.3680 - Val Loss: 0.5968 - IOU: 0.2467 - F1: 0.2892\n",
      "  Loss Components - Focal: 0.0485, IoU: 0.1966, Boundary: 0.0745\n",
      "Epoch 20/100 - Train Loss: 0.3745 - Val Loss: 0.6545 - IOU: 0.2247 - F1: 0.2687\n",
      "  Loss Components - Focal: 0.0500, IoU: 0.1997, Boundary: 0.0744\n",
      "Epoch 21/100 - Train Loss: 0.3312 - Val Loss: 0.6310 - IOU: 0.2251 - F1: 0.2683\n",
      "  Loss Components - Focal: 0.0453, IoU: 0.1746, Boundary: 0.0745\n",
      "Epoch 22/100 - Train Loss: 0.3390 - Val Loss: 0.5904 - IOU: 0.2319 - F1: 0.2755\n",
      "  Loss Components - Focal: 0.0461, IoU: 0.1792, Boundary: 0.0745\n",
      "Epoch 23/100 - Train Loss: 0.3559 - Val Loss: 0.6112 - IOU: 0.2318 - F1: 0.2754\n",
      "  Loss Components - Focal: 0.0497, IoU: 0.1876, Boundary: 0.0744\n",
      "Epoch 24/100 - Train Loss: 0.2741 - Val Loss: 0.6622 - IOU: 0.2315 - F1: 0.2773\n",
      "  Loss Components - Focal: 0.0381, IoU: 0.1423, Boundary: 0.0744\n",
      "Epoch 25/100 - Train Loss: 0.3117 - Val Loss: 0.6074 - IOU: 0.2415 - F1: 0.2836\n",
      "  Loss Components - Focal: 0.0462, IoU: 0.1609, Boundary: 0.0744\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Choose your training method:\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Method A: Use predefined scenario (RECOMMENDED)\u001b[39;00m\n\u001b[32m     14\u001b[39m loss_config = \u001b[33m\"\u001b[39m\u001b[33mimbalanced\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Choose: \"balanced\", \"imbalanced\", \"noisy\", \"boundary_critical\", \"original\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m           \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m           \u001b[49m\u001b[43mfreeze_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m           \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m           \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m           \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m           \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m           \u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Method B: Use custom parameters (ADVANCED)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# custom_loss_weights = {'focal': 1.5, 'iou': 1.2, 'boundary': 0.3}\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# custom_focal_params = {'alpha': 0.85, 'gamma': 3.0}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[38;5;66;03m#            loss_weights=suggested['loss_weights'],\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m#            focal_params=suggested['focal_params'])\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/radburst_tl/training/train_utils.py:547\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, initial_lr, freeze_epochs, total_epochs, checkpoint_dir, patience, device, loss_weights, focal_params, loss_config)\u001b[39m\n\u001b[32m    545\u001b[39m freeze_encoder_weights(model)\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, freeze_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     train_loss, train_components = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocal_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m     val_loss, val_metrics = validate_one_epoch(model, val_loader, device, loss_weights, focal_params)\n\u001b[32m    549\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfreeze_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - IOU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33miou\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/radburst_tl/training/train_utils.py:367\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device, loss_weights, focal_params)\u001b[39m\n\u001b[32m    365\u001b[39m x, y = x.to(device), y.to(device)\n\u001b[32m    366\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Use enhanced loss function\u001b[39;00m\n\u001b[32m    370\u001b[39m loss = combined_loss(y, preds, \n\u001b[32m    371\u001b[39m                    loss_weights=loss_weights,\n\u001b[32m    372\u001b[39m                    focal_params=focal_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/segmentation_models_pytorch/base/model.py:49\u001b[39m, in \u001b[36mSegmentationModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_input_shape(x)\n\u001b[32m     48\u001b[39m features = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m decoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m masks = \u001b[38;5;28mself\u001b[39m.segmentation_head(decoder_output)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:122\u001b[39m, in \u001b[36mUnetDecoder.forward\u001b[39m\u001b[34m(self, *features)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, decoder_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.blocks):\n\u001b[32m    121\u001b[39m     skip = skips[i] \u001b[38;5;28;01mif\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(skips) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     x = \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:43\u001b[39m, in \u001b[36mDecoderBlock.forward\u001b[39m\u001b[34m(self, x, skip)\u001b[39m\n\u001b[32m     41\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.attention1(x)\n\u001b[32m     42\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv1(x)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m x = \u001b[38;5;28mself\u001b[39m.attention2(x)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Enhanced Training with Configurable Loss Parameters\n",
    "# initial_lr = 1e-3\n",
    "# freeze_epochs = 100\n",
    "# total_epochs = 150\n",
    "# patience = 10\n",
    "# checkpoint_dir = './checkpoints_enhanced'\n",
    "\n",
    "print(\"üöÄ Starting Enhanced Training with Configurable Loss...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Choose your training method:\n",
    "\n",
    "# # Method A: Use predefined scenario (RECOMMENDED)\n",
    "# loss_config = \"imbalanced\"  # Choose: \"balanced\", \"imbalanced\", \"noisy\", \"boundary_critical\", \"original\"\n",
    "\n",
    "# train_model(model, train_loader, val_loader, \n",
    "#            initial_lr=initial_lr,\n",
    "#            freeze_epochs=freeze_epochs, \n",
    "#            total_epochs=total_epochs,\n",
    "#            checkpoint_dir=checkpoint_dir, \n",
    "#            patience=patience, \n",
    "#            device=device,\n",
    "#            loss_config=loss_config)\n",
    "\n",
    "# Method B: Use custom parameters (ADVANCED)\n",
    "# custom_loss_weights = {'focal': 1.5, 'iou': 1.2, 'boundary': 0.3}\n",
    "# custom_focal_params = {'alpha': 0.85, 'gamma': 3.0}\n",
    "# \n",
    "# train_model(model, train_loader, val_loader, \n",
    "#            initial_lr=initial_lr,\n",
    "#            freeze_epochs=freeze_epochs, \n",
    "#            total_epochs=total_epochs,\n",
    "#            checkpoint_dir=checkpoint_dir, \n",
    "#            patience=patience, \n",
    "#            device=device,\n",
    "#            loss_weights=custom_loss_weights,\n",
    "#            focal_params=custom_focal_params)\n",
    "\n",
    "# Train with enhanced loss configuration\n",
    "trained_model, history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    initial_lr=training_config_deeplabv3['initial_lr'],\n",
    "    freeze_epochs=training_config_deeplabv3['freeze_epochs'],  # 0 for DeepLabV3+\n",
    "    total_epochs=training_config_deeplabv3['total_epochs'],\n",
    "    checkpoint_dir='./checkpoints_deeplabv3',  # Separate directory\n",
    "    patience=training_config_deeplabv3['patience'],\n",
    "    device=device,\n",
    "    loss_config=training_config_deeplabv3['loss_config']  # Use imbalanced config\n",
    ")\n",
    "\n",
    "# Method B: Custom DeepLabV3+ parameters (ADVANCED)\n",
    "# custom_loss_weights = {'focal': 0.8, 'iou': 1.0, 'boundary': 0.2}  # Adjust weights\n",
    "# custom_focal_params = {'alpha': 0.85, 'gamma': 2.5}                # Fine-tune focal loss\n",
    "# custom_backbone = 'resnet18'  # 'resnet18', 'resnet34', 'efficientnet-b0'\n",
    "# \n",
    "# # Rebuild model with custom backbone if needed\n",
    "# # model = build_deeplabv3(encoder_name=custom_backbone, encoder_weights=None)\n",
    "# # model.to(device)\n",
    "# \n",
    "# trained_model, history = train_model(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     initial_lr=training_config_deeplabv3['initial_lr'],\n",
    "#     freeze_epochs=training_config_deeplabv3['freeze_epochs'],  # 0 for DeepLabV3+\n",
    "#     total_epochs=training_config_deeplabv3['total_epochs'],\n",
    "#     checkpoint_dir='./checkpoints_deeplabv3',  # Separate directory\n",
    "#     patience=training_config_deeplabv3['patience'],\n",
    "#     device=device,\n",
    "#     loss_weights=custom_loss_weights,\n",
    "#     focal_params=custom_focal_params\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Enhanced vs Original Loss Comparison\n",
    "\n",
    "Load and Evaluate the Best Model\n",
    "\n",
    "After training, load the best saved checkpoint and evaluate the model on the validation set.\n",
    "\n",
    "You can compare the enhanced loss function with the original BCE+IoU loss to see the improvement:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best enhanced model and compare with original loss\n",
    "print(\"üîç Comparing Enhanced vs Original Loss Functions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the enhanced model\n",
    "best_enhanced_path = './checkpoints_enhanced/[your_best_checkpoint].pth'  # Update this path\n",
    "# checkpoint = torch.load(best_enhanced_path, map_location=device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Evaluate with enhanced loss (already loaded)\n",
    "print(\"üìä Enhanced Loss Results:\")\n",
    "val_loss_enhanced, val_metrics_enhanced = validate_one_epoch(model, val_loader, device)\n",
    "print(f\"  Validation Loss: {val_loss_enhanced:.4f}\")\n",
    "print(f\"  IoU: {val_metrics_enhanced['iou']:.4f}\")\n",
    "print(f\"  F1:  {val_metrics_enhanced['f1']:.4f}\")\n",
    "\n",
    "# For comparison, evaluate with original loss\n",
    "print(\"\\\\nüìä Original Loss (BCE+IoU) for comparison:\")\n",
    "model.eval()\n",
    "total_original_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = model(x)\n",
    "        # Use original simple loss\n",
    "        original_loss = simple_combined_loss(y, preds)\n",
    "        total_original_loss += original_loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "avg_original_loss = total_original_loss / num_batches\n",
    "print(f\"  Original Loss: {avg_original_loss:.4f}\")\n",
    "print(f\"  IoU: {val_metrics_enhanced['iou']:.4f} (same model)\")\n",
    "print(f\"  F1:  {val_metrics_enhanced['f1']:.4f} (same model)\")\n",
    "\n",
    "# Show improvement\n",
    "improvement = avg_original_loss - val_loss_enhanced\n",
    "improvement_pct = improvement / avg_original_loss * 100\n",
    "print(f\"\\\\nüéØ Loss Improvement: {improvement:.4f} ({improvement_pct:+.1f}%)\")\n",
    "\n",
    "# Individual loss component analysis\n",
    "print(f\"\\\\nüß© Enhanced Loss Component Breakdown:\")\n",
    "with torch.no_grad():\n",
    "    sample_x, sample_y = next(iter(val_loader))\n",
    "    sample_x, sample_y = sample_x.to(device), sample_y.to(device)\n",
    "    sample_preds = model(sample_x)\n",
    "    \n",
    "    focal_val = focal_loss(sample_preds, sample_y, alpha=0.8, gamma=2.5)\n",
    "    iou_val = adaptive_iou_loss(sample_preds, sample_y, power=1.5)\n",
    "    boundary_val = boundary_loss(sample_preds, sample_y)\n",
    "    \n",
    "    print(f\"  Focal Loss:    {focal_val.item():.4f}\")\n",
    "    print(f\"  IoU Loss:      {iou_val.item():.4f}\")\n",
    "    print(f\"  Boundary Loss: {boundary_val.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
