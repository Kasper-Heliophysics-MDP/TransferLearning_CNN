{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a UNet Model for Solar Radio Burst Segmentation using PyTorch\n",
    "\n",
    "In this notebook, we demonstrate how to train a UNet model for segmenting solar radio bursts using transfer learning. \n",
    "The training is split into two phases:\n",
    "\n",
    "1. **Phase 1:** Freeze the encoder (pre-trained on ImageNet) and train only the decoder.  \n",
    "2. **Phase 2:** Unfreeze the encoder and fine-tune the entire model using a lower learning rate.\n",
    "\n",
    "We use a combined loss function consisting of binary cross-entropy (BCE) loss and a Jaccard (IOU) loss, and we monitor the IOU and F1 metrics on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from train_utils import create_dataset, build_unet, freeze_encoder_weights, unfreeze_encoder_weights, combined_loss, compute_metrics, train_one_epoch, validate_one_epoch, adjust_learning_rate, save_checkpoint, train_model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Loading and Preprocessing\n",
    "\n",
    "We assume that image and mask CSV files are stored in a single directory.\n",
    "The naming convention is as follows:\n",
    "\n",
    "- For burst slices:\n",
    "       slice_20240608_y155_x270406_SkylineHS.csv\n",
    "       slice_20240608_y155_x270406_SkylineHS_mask.csv\n",
    "\n",
    "- For non-burst slices:\n",
    "       slice_20240420_y0_x3391_PeachMountain_2020_nonburst.csv\n",
    "\n",
    "The `create_dataset` function reads the files, normalizes them to [0, 1], and splits the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (1171, 256, 256, 1)\n",
      "Validation images shape: (293, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/remiliascarlet/Desktop/MDP/transfer_learning/burst_data/csv/saved_slices/finished'\n",
    "\n",
    "(train_images, train_masks), (val_images, val_masks) = create_dataset(data_dir, img_size=(256, 256), test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create DataLoaders\n",
    "\n",
    "Convert the numpy arrays into PyTorch tensors and create DataLoaders.\n",
    "\n",
    "We also need to permute dimensions from (N, H, W, C) to (N, C, H, W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert numpy arrays to Torch tensors and permute to (N, channels, H, W)\n",
    "train_images_tensor = torch.tensor(train_images).permute(0, 3, 1, 2)\n",
    "train_masks_tensor  = torch.tensor(train_masks).permute(0, 3, 1, 2)\n",
    "\n",
    "val_images_tensor = torch.tensor(val_images).permute(0, 3, 1, 2)\n",
    "val_masks_tensor  = torch.tensor(val_masks).permute(0, 3, 1, 2)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_images_tensor, train_masks_tensor)\n",
    "val_dataset   = TensorDataset(val_images_tensor, val_masks_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Construction\n",
    "\n",
    "Build a UNet model using the `build_unet` function from the segmentation_models library.\n",
    "\n",
    "Here we set `input_shape=(256,256,1)` for grayscale images, `num_classes=1` for binary segmentation,\n",
    "and use pre-trained ImageNet weights with a ResNet34 backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Please install the segmentation_models library: pip install segmentation-models",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/segmentation_models/__init__.py:98\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[43mset_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_framework\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/segmentation_models/__init__.py:67\u001b[39m, in \u001b[36mset_framework\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == _KERAS_FRAMEWORK_NAME:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mefficientnet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m  \u001b[38;5;66;03m# init custom objects\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/radburst_tl/training/train_utils.py:99\u001b[39m, in \u001b[36mbuild_unet\u001b[39m\u001b[34m(input_shape, num_classes, encoder_weights)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msegmentation_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Unet\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/segmentation_models/__init__.py:101\u001b[39m\n\u001b[32m    100\u001b[39m     other = _TF_KERAS_FRAMEWORK_NAME \u001b[38;5;28;01mif\u001b[39;00m _framework == _KERAS_FRAMEWORK_NAME \u001b[38;5;28;01melse\u001b[39;00m _KERAS_FRAMEWORK_NAME\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[43mset_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSegmentation Models: using `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m` framework.\u001b[39m\u001b[33m'\u001b[39m.format(_KERAS_FRAMEWORK))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/.venv/lib/python3.13/site-packages/segmentation_models/__init__.py:70\u001b[39m, in \u001b[36mset_framework\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name == _TF_KERAS_FRAMEWORK_NAME:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mefficientnet\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtfkeras\u001b[39;00m  \u001b[38;5;66;03m# init custom objects\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import segmentation_models_pytorch library if not already installed: pip install segmentation-models-pytorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mbuild_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimagenet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model.to(device)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MDP/transfer_learning/radburst_tl/training/train_utils.py:101\u001b[39m, in \u001b[36mbuild_unet\u001b[39m\u001b[34m(input_shape, num_classes, encoder_weights)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msegmentation_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Unet\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease install the segmentation_models library: pip install segmentation-models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m model = Unet(\n\u001b[32m    104\u001b[39m     backbone_name=\u001b[33m'\u001b[39m\u001b[33mresnet34\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;66;03m# resnet34, resnet50, resnet101, resnet152\u001b[39;00m\n\u001b[32m    105\u001b[39m     input_shape=input_shape,\n\u001b[32m    106\u001b[39m     classes=num_classes,\n\u001b[32m    107\u001b[39m     encoder_weights=encoder_weights\n\u001b[32m    108\u001b[39m )\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[31mImportError\u001b[39m: Please install the segmentation_models library: pip install segmentation-models"
     ]
    }
   ],
   "source": [
    "# Import segmentation_models_pytorch library if not already installed: pip install segmentation-models-pytorch\n",
    "model = build_unet(input_shape=(256, 256, 1), num_classes=1, encoder_weights='imagenet')\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training the Model\n",
    "\n",
    "We train the model using a two-phase process:\n",
    "\n",
    "    - **Phase 1:** The encoder is frozen for a number of epochs, training only the decoder.\n",
    "\n",
    "    - **Phase 2:** The encoder is unfrozen, the learning rate is reduced, and the entire model is fine-tuned.\n",
    "\n",
    "Early stopping is applied if no improvement is observed for a given number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-3\n",
    "freeze_epochs = 10    # Adjust as needed.\n",
    "total_epochs = 20\n",
    "patience = 3\n",
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "# Train the model using the previously defined `train_model` function.\n",
    "train_model(model, train_loader, val_loader, initial_lr=initial_lr,\n",
    "            freeze_epochs=freeze_epochs, total_epochs=total_epochs,\n",
    "            checkpoint_path=checkpoint_dir, patience=patience, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load and Evaluate the Best Model\n",
    "\n",
    "After training, load the best saved checkpoint and evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'xxxxx'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Loaded best model from checkpoint.\")\n",
    "\n",
    "# Evaluate the model on the validation DataLoader\n",
    "val_loss, val_metrics = validate_one_epoch(model, val_loader, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, IOU: {val_metrics['iou']:.4f}, F1: {val_metrics['f1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
